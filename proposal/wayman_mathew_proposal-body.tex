\section{Problem description}

Until recently, recurrent neural networks (RNNs), used for sequential processing, were limited by the fact that within-sequence memory was limited to short-term only (long-term memory was limited to between-sequence memory). The ``fast weights'' method introduced in the paper to be analyzed in this project \cite{NIPS2016_6057} addresses this limitation by providing the network with the capacity to store information about a given sequence during its duration (to be used during each step in the hidden layers). We will provide a full analysis and explanation of the methology, and replicate one of the empirical tests of the method, which compares its performance on an associative retrieval task to that of an iRNN and an LSTM.

\section{Survey of prior work}

Recurrent neural networks (RNNs) are well-suited for learning from sequential data since weights are shared among different stages of the sequence \cite[p. 373]{Goodfellow-et-al-2016}. In particular, Recurrent Neural Nets have been shown to perform well in tasks of Speech to Text conversion, creation of Language models for both characters and words \cite{sutskever} and even frame by frame video analyses \cite{mnih}. In RNNs, hidden states essentially act as short-term memory for previous states with inputs and hidden states helping to define the input and future hidden state. One major issue in training RNNs with many layers was that the gradients (of the error with respect to a particular weight) end up becoming very large or small \cite[p. 16]{DBLP:journals/nn/Schmidhuber15}. This was overcome by the introduction of the long short-term memory network (LSTM RNN), whose activation function has a constant derivative and thus does not explode or vanish \cite[p. 19]{DBLP:journals/nn/Schmidhuber15}.

\{Next: explain weakness of LSTMs and how introduction of fast associative memory addresses that weakness.\}
LSTM networks are composed of memory blocks, each of which contain 'memory cells'. Each cell has an activation over a unit and is referred to as the 'cell state'. Due to this linear nature of cell connections, cell states are prone to grow in a linear manner as time units are expended. There have been many attempts to overcome this and other shortcomings of LSTMs. {see last citation}

Hopfield nets, associative memory: \cite{Mackay03informationtheory}

Layer normalization: \cite{1607.06450}

Grid search: \cite{Goodfellow-et-al-2016}

Adam optimizer: \cite{1412.6980}

IRNN definition: \cite{1511.03771}

LSTM: \cite{DBLP:journals/neco/GersSC00}, \cite{hochreiter1997}

\section{Preliminary plan}

Our term paper will first present the fast associative memory methodology and place it in the context of methods that led to its development. We will provide an extended description and derivation of the methodology for the purpose of verifying its properties. Our goal will be to also replicate section 4.1 of the paper, which compares the fast associative memory method's performance on an associative retrieval task with that of an Identity-RNN (iRNN) and LSTM \cite{NIPS2016_6057}.

This project is intended to understand the foundational math and reasoning behind pursuing the use of Fast Weights in LSTM networks. To do so, we must first perform a thorough proof and derivation of the research and developments that led to the creation of LSTM networks. This will form the initial stage of our project. Some of the components that need to be proven include the following:
\begin{itemize}
  \item
  Speed at which weight matrix is diminished by BPTT corresponds to the shortness of memory.
  \item
  Given that expression of Hidden unit at time t is: \\
  \begin{equation}
    h_t = g(\textbf W \cdot x_t + \textbf U \cdot h_t-1 + b_h)
  \end{equation}
  after $t$ time steps, we get : \\
  \begin{equation}
    h_t = g(\textbf W \cdot x_t + \textbf U \cdot g(\cdots g(\textbf W \cdot x_t-T + U \cdot h_t-T + b_h) \cdots) + b_h)
  \end{equation}
\end{itemize}

Because of the $T$ nested multiplications by $\textbf U$, the effect of $h_t-T$ ("Long Term Memory") is negligible.

\begin{table*}[t]
  \caption{Project timeline}
  \label{tab:freq}
  \begin{tabular}{llll}
    \toprule
    Week & Dates & Task & Deliverable\\
    \midrule
    Week 1 & (2/6   - 2/13) & Background Reading & \\
    Week 2 & (2/13  - 2/20) & Background Reading & Proposal\\
    Week 3 & (2/20  - 2/27) & Data collection & \\
    Week 4 & (2/27  - 3/6)  & Background, foundational proofs & \\
    Week 5 & (3/6   - 3/13) & Compile data and preliminary run & \\
    Week 6 & (3/20  - 3/27) & Report prep & Intermediate Report\\
    Week 7 & (4/3   - 4/10) & Full implementation & \\
    Week 8 & (4/10  - 4/17) & Run with Data and comparisons & \\
    Week 9 & (4/17  - 4/24) & Report prep & \\
    Week 10 & (4/24 - 5/1) & Report prep + Rehearse & Final Report \\
    & & & Presentation\\
  \bottomrule
\end{tabular}
\end{table*}

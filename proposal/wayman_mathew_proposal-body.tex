\section{Problem description}

Until recently, recurrent neural networks (RNNs), used for sequential processing, were limited by the fact that within-sequence memory was limited to short-term only (long-term memory was limited to between-sequence memory). The ``fast weights'' method introduced in the paper to be analyzed in this project \cite{NIPS2016_6057} addresses this limitation by providing the network with the capacity to store information about a given sequence during its duration (to be used during each step in the hidden layers). We will provide a full analysis and explanation of the methology, and replicate one of the empirical tests of the method, which compares its performance on an associative retrieval task to that of an iRNN and an LSTM.

\section{Survey of prior work}

Recurrent neural networks (RNNs) are well-suited for learning from sequential data since weights are shared among different stages of the sequence \cite[p. 373]{Goodfellow-et-al-2016}. In particular, Recurrent Neural Nets have been shown to perform well in tasks of Speech to Text conversion, creation of Language models for both characters and words \cite{sutskever} and even frame by frame video analyses \cite{mnih}. In RNNs, hidden states essentially act as short-term memory for previous states with inputs and hidden states helping to define the input and future hidden state. One major issue in training RNNs with many layers was that the gradients (of the error with respect to a particular weight) end up becoming very large or small \cite[p. 16]{DBLP:journals/nn/Schmidhuber15}. This was overcome by the introduction of the long short-term memory network (LSTM RNN), whose activation function has a constant derivative and thus does not explode or vanish \cite[p. 19]{DBLP:journals/nn/Schmidhuber15}.

\{Next: explain weakness of LSTMs and how introduction of fast associative memory addresses that weakness.\}

Hopfield nets, associative memory: \cite{Mackay03informationtheory}

Layer normalization: \cite{1607.06450}

Grid search: \cite{Goodfellow-et-al-2016}

Adam optimizer: \cite{1412.6980}

IRNN definition: \cite{1511.03771}

\section{Preliminary plan}

Our term paper will first present the fast associative memory methodology and place it in the context of methods that led to its development. We will provide an extended description and derivation of the methodology for the purpose of verifying its properties. Our goal will be to also replicate section 4.1 of the paper, which compares the fast associative memory method's performance on an associative retrieval task with that of an Identity-RNN (iRNN) and LSTM.


\begin{table*}[t]
  \caption{Project timeline}
  \label{tab:freq}
  \begin{tabular}{llll}
    \toprule
    Week & Dates & Task & Deliverable\\
    \midrule
    Week 1 & (2/6   - 2/13) & Background Reading & \\
    Week 2 & (2/13  - 2/20) & Background Reading & Proposal\\
    Week 3 & (2/20  - 2/27) & Data collection & \\
    Week 4 & (2/27  - 3/6)  & Background, foundational proofs & \\
    Week 5 & (3/6   - 3/13) & Compile data and preliminary run & \\
    Week 6 & (3/20  - 3/27) & Report prep & Intermediate Report\\
    Week 7 & (4/3   - 4/10) & Full implementation & \\
    Week 8 & (4/10  - 4/17) & Run with Data and comparison & \\
    Week 9 & (4/17  - 4/24) & Report prep & \\
    Week 10 & (4/24 - 5/1) & Report prep + Rehearse & Final Report \\
    & & & Presentation\\
  \bottomrule
\end{tabular}
\end{table*}

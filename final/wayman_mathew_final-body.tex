\section{Problem description}

Until recently, recurrent neural networks (RNNs), used for sequential processing, did not have a good way to share memory between sequences.
The Fast Weights method introduced in the paper to be analyzed in this project \cite{DBLP:conf/nips/BaHMLI16} addresses this limitation by providing the network with the capacity to store information about a given sequence during its duration to be used in the upcoming sequence. We will provide a full analysis and explanation of the methology, and replicate one of the empirical tests of the method, which compares its performance on an associative retrieval task to that of an iRNN and a long short-term memory network, or LSTM \cite{DBLP:journals/neco/HochreiterS97}.

\section{Survey of prior work}

Recurrent neural networks (RNNs) are well-suited for learning from sequential data since weights are shared among different stages of the sequence \cite[p. 373]{Goodfellow-et-al-2016}. In particular, RNNs have been shown to perform well in tasks of speech-to-text conversion, creation of language models for both characters and words \cite{DBLP:conf/icml/SutskeverMH11} and even frame by frame video analyses \cite{mnih}. In RNNs, a given hidden state essentially acts as short-term memory for the previous state, determining the next hidden state together with the next input. One major issue in training RNNs with long input sequences is that the error gradients end up becoming very large or small \cite[p. 16]{DBLP:journals/nn/Schmidhuber15} which implies that even if the network can be trained, the effect of an early hidden state on the current hidden state is practically non-existent. This problem was overcome by the introduction of the long short-term memory RNN (LSTM RNN), whose activation function has a constant derivative and thus does not explode or vanish \cite[p. 19]{DBLP:journals/nn/Schmidhuber15}. Unfortunately, the LSTM RNN's memory is still limited to an amount proportional to the number of hidden units in a sequence \cite[p. 1]{DBLP:conf/nips/BaHMLI16}. Ba et al. propose the Fast Weights method to allow sequence-to-sequence memory in a recurrent network. We also note that Hopfield nets \cite{MacKay:2002:ITI:971143} implemented a similar storage rule \cite[p. 2]{DBLP:conf/nips/BaHMLI16} which we will review in our paper.

\section{Preliminary plan}

Our term paper will first present the Fast Weights methodology and place it in the context of methods that led to its development. We will provide an extended description and derivation of the methodology for the purpose of verifying its properties. Our goal will be to also replicate Section 4.1 of the paper, which compares the Fast Weights' performance on an associative retrieval task with that of an Identity-RNN (iRNN) \cite{DBLP:journals/corr/TalathiV15}, and an LSTM RNN \cite{DBLP:conf/nips/BaHMLI16}.

This project is intended to verify the foundational math and reasoning which justify the use of Fast Weights in a network. This will require us to retrace the work leading up to the introduction of Fast Weights. The initial stage of our project will be to perform a thorough proof and derivation of the equations for RNNs, and clearly explain the issues that led to the creation of LSTM RNNs. For instance, we will explain the ``long-term memory issue'' in RNNs beginning as follows. The expression of the hidden unit $h_t$ at time $t$ is:

\begin{equation*}
  h_t = g(\vect{W} \cdot x_t + \vect{U} \cdot h_{t-1} + b_h)
\end{equation*}

After $t$ time steps, we get:

\begin{equation*}
  h_t = g(\vect{W} \cdot x_t + \vect{U} \cdot g(\cdots g(\vect{W} \cdot x_{t-T} + \vect{U} \cdot h_{t-T} + b_h) \cdots) + b_h)
\end{equation*}

Because of the $T$ nested multiplications of $h_{t-T}$ by $\vect{U}$, the effect of $h_{t-T}$ on $h_t$ is negligible (namely, the network does not have ``long-term memory''). We will provide a full exposition of how this problem manifests itself when the network is trained.

The next stage of the project will involve explaining LSTM RNNs, their improvements over RNNs, and their limitations. We will then explain the mathematics of Fast Weights in RNNs, as well as several methodologies used in their implementation in the paper being studied such as layer normalization \cite{DBLP:journals/corr/BaKH16}, grid search \cite{Goodfellow-et-al-2016}, and the Adam optimizer \cite{DBLP:journals/corr/KingmaB14}.

Following that, we will implement the Fast Associative Memory Network in MATLAB, and reproduce the analysis of Section 4.1 \cite{DBLP:conf/nips/BaHMLI16} of the paper to confirm the performance of Fast Weights as compared to the iRNN and the LSTM RNN.

\section{Memory and recurrent neural networks}

Paragraph about the neural perspective.

Paragraph about treating as signals, etc.

There are two major ways of incorporating the temporal element of sequential data in neural network learning. One is providing a memory structure that can present data from multiple time steps to a network that does not itself depend on the time variable (e.g. the time element is handled externally) \cite[p. 672-673]{Haykin:2009:NNC:1213811}. The other is incorporating the time element directly inside the network via the use of feedback. Feedback occurs in a system when the output of an element of the system eventually affects the input to that same element \cite[p. 18]{Haykin:2009:NNC:1213811}. There are two main types of feedback: local feedback and global feedback. Local feedback occurs when the output from an element directly feeds into that element's input, and global feedback occurs when the output eventually affects the input after passing through other elements first \cite[p. 673]{Haykin:2009:NNC:1213811}.

Recurrent neural networks are networks containing at least one feedback loop of either type \cite[p. 23]{Haykin:2009:NNC:1213811}. Feedback loops are inherently time-delay elements, where the output from an element is fed into a new element with a delay (in other words, transmission from one node to another is not instantaneous).

One major use of recurrent neural networks is to provide \emph{associative memory}. In the storage phase, an associative memory is presented with \emph{key patterns} and stores memorized patterns or \emph{values} which are implicitly associated with their key patterns. In the recall phase, when presented with a distorted or incomplete version of a key pattern, the memory produces the associated value pattern \cite[p. 38]{Haykin:2009:NNC:1213811}.

Let $s + l = N$, where $s$ is the total number of neurons used for storage of patterns and $l$ is the number of neurons that can be used for learning. One metric used when designing an associative memory is the ratio $q = \frac{s}{N} = \frac{s}{s + l}$. The total machine capacity ($N$) is limited to some value by resource constraints. Note that for a small $q$, the performance may be very good, though $s$ is small relative to $l$: in other words, the network must use a large number of neurons $l$ to recall very well only a few patterns. Therefore we seek to make $q$ as large as possible while still achieving good performance, i.e. the network should be able to recall correctly many patterns by using as few neurons for learning as possible \cite[p. 39]{Haykin:2009:NNC:1213811}.

There are two main types of associative memory, \emph{autoassociative} and \emph{heteroassociative memory}: in autoassociative memory, the memorized patterns are the same as the key patterns, whereas in heteroassociative memory, the memorized patterns are different \cite[p. 38]{Haykin:2009:NNC:1213811}. Different network structures may be more suited for one task than the other.

\subsection{A simple example of an associative memory model}

Consider an associative memory which learns the single key pattern $f$ and value $g$ where both are column vectors (this example is from \cite[p. 163-165]{Anderson95}). We let the system be the matrix

\begin{equation*}
A = \eta g f^T
\end{equation*}

The system performs perfectly:

\begin{equation*}
g^\prime = Af = \eta g f^T f \propto g
\end{equation*}
%
since the $g^\prime$ that is recalled is proportional to the value $g$ associated with the input $f$.

Now consider a set of key patterns $f_i$ and associated values $g_i$ where all $f_i$ are orthogonal (we write $f_i \rightarrow g_i$ to denote the associations). Letting

\begin{equation*}
A_i = g_i f_i^T, \qquad A = \sum_{i} A_i
\end{equation*}
%
we see that again $A$ performs recall perfectly since for all $j$,

\begin{align*}
  A f_j & = \sum_{i}A_i f_j = \sum_{k \neq j} A_k f_j + A_j f_j \\
  & = \sum_{k \neq j} g_k f_k^T f_j + \eta g_j \propto g_j
\end{align*}

The above example suggests that outer products can be useful in constructing associative memory models. Using outer products to create memory storage is referred to ``the generalization of Hebb's postulate of learning'' \cite[p. 698]{Haykin:2009:NNC:1213811} since weight updates in Hebbian learning are calculated with outer products, as in the following example for a one-layer network \cite[p. 39-40]{fyfe2000}. For output $y \in \mathbb{R}^m$ and input $x \in \mathbb{R}^n$, if

\begin{equation*}
  y_i = \sum_{j} w_{ij} x_j, \qquad \Delta w_{ij} = \alpha x_j y_i
\end{equation*}
%
then

\begin{equation*}
\Delta w_{ij} = \alpha x_j \sum_k w_{ik} x_k = \alpha \sum_k w_{ik}x_j x_k
\end{equation*}

Letting $\alpha = \Delta t$,

\begin{equation*}
  \frac{\Delta w_{ij}}{\Delta t} = \sum_k w_{ik}x_j x_k
\end{equation*}
%
so as $\Delta t \rightarrow 0$,

\begin{equation*}
\frac{d}{dt}W(t) = C W(t)
\end{equation*}
%
or, writing out the matrices fully,

\begin{equation*}
  \begin{split}
  \begin{bmatrix}
    \frac{dw_{11}}{dt} & \cdots & \frac{dw_{m1}}{dt} \\
    \vdots & \vdots & \vdots \\
    \frac{dw_{1n}}{dt} & \cdots & \frac{dw_{mn}}{dt} \\    
  \end{bmatrix} = \begin{bmatrix}
    x_1 x_1 & \cdots & x_n x_1 \\
    \vdots & \cdots & \vdots \\
    x_1 x_n & \cdots & x_n x_n 
  \end{bmatrix} \\
  & \hspace{-1.0cm}\begin{bmatrix}
    w_{11} & \cdots & w_{m1} \\
    \vdots & \cdots & \vdots \\
    w_{1n} & \cdots & w_{mn}
    \end{bmatrix}
  \end{split}
\end{equation*}

Note that the simple example of autoassociative memory would correspond to the weights matrix initially being the zero matrix and having only one update, after which the memory can recall perfectly. In general the linear system just described will not converge without some constraints being imposed \cite[p. 40]{fyfe2000}. Since it is known that given orthogonal key patterns only one weight update is needed, the algorithm can be run without considering issues of convergence or stability of the general case. However, stability of such systems is essential for learning algorithms based in such systems to exist; we will consider this point below in more detail.

Since not all sets of key values are orthogonal, the above system is inadequate for most cases. Before moving to other models we provide an overview of stability issues necessary for the design and evaluation of recurrent networks.

\section{Stability of dynamic systems}

A \emph{dynamic system} is a system whose state changes with time \cite[p. 675]{Haykin:2009:NNC:1213811}. Above it was noted that feedback is a way of introducing time lags into a dynamic system. Depending on the particulars of the system, feedback can lead a system to stability or cause it to diverge. We consider a simple example \cite[p. 18-21]{Haykin:2009:NNC:1213811} of a dynamic system with feedback. From this example we will see that if the operator mapping input to output is a weight matrix, and outputs are mapped to inputs through a linear additive function, the values of the weights will determine whether the system is stable or diverges.

Consider the system defined by
%
\begin{equation*}
y_k(n) = wx_j^\prime(n), \qquad x_j^\prime(n) = x_j(n) + z^{-1}[y_k(n)]
\end{equation*}
%
where $z^{-1}$ is the unit time-delay operator, so $z^{-1}[y_k(n)] = y_k(n-1)$. Combining the two equations gives

\begin{equation*}
  y_k(n) = w \left(x_j(n) + z^{-1}[y_k(n)]\right)
\end{equation*}

\begin{equation*}
  y_k(n)(1 - wz^{-1}) = wx_j(n)
\end{equation*}

\begin{equation*}
  y_k(n) = w(1 - wz^{-1})^{-1}x_j(n)
\end{equation*}

Since
%
\begin{align*}
  \sum_{l=0}^{\infty}w^l z^{-l} & = \sum_{l=0}^{\infty}\left(\frac{w}{z}\right)^l = \frac{1}{(1 - wz^{-1})} \\
  & = (1 - wz^{-1})^{-1}
\end{align*}
%
we write
%
\begin{equation*}
  y_k(n) = w \sum_{l=0}^{\infty}w^l z^{-l}x_j(n) = \sum_{l=0}^{\infty}w^{l+1}x_j(n - l)
\end{equation*}
%
where we have applied the unit-time delay operator to the $x_j$ term.

We consider the case where $x_j(k)$ are sampled from, say, a Gaussian distribution whose positive mean is much smaller than the value of $x_j(0)$. We observe three cases:

\begin{enumerate}
\item If $|w| < 1$, the effect of the signal will decay towards $0$.
\item If $w = 1$, the system will diverge with the trend of $y_k(n)$ being linear. 
\item If $w > 1$, the system will diverge with the trend of $y_k(n)$ being exponential.  
\end{enumerate}

\subsection{Stability for autonomous dynamic systems}

We note that system just described is a difference equation that is \emph{linear} (namely, the exponents of $x_j$ are to the first power only). Recurrent neural networks are generally \emph{nonlinear} (the exponents of $x_j$ are to powers greater than one \cite[p. 6]{strogatz:2000} or the $x_j$ are not in polynomial form). We present some important definitions here regarding dynamic systems:

\begin{definition}
  State variables: $x_1(t), \ldots, x_N(t)$. Independent variable: $t$. Order of the system: $N$. State vector (an $N$-dimensional column vector): $x(t)$.
\end{definition}

A nonlinear dynamic system thus consists of the equations
%
\begin{equation*}
  \frac{d}{dt}x_j(t) = F_j \left(x_j(t)\right), \qquad j = 1, 2, \ldots, N
\end{equation*}
%
or
%
\begin{equation*}
  \frac{d}{dt}x(t) = F \left(x(t)\right)
\end{equation*}
%
where $F$ is non-linear and vector valued, with each element depending on the corresponding element of $x(t)$. If $F$ only depends on $t$ through $x$, the system is \emph{autonomous}, otherwise it is \emph{nonautonomous} \cite[p. 675]{Haykin:2009:NNC:1213811}). The above equation is called the \emph{state-space} equation (ibid.). We wish to establish the existence and possibly uniqueness of solutions to the state-space equation. This is easier to do with autonomous than nonautonomous systems \cite[p. 180]{DBLP:journals/ai/Beer95}.

Let be $F$ autonomous. Then $F$ continuous is sufficient for the existence of a solution (note that this is not true for nonautonomous systems). For the uniqueness of the solution we require the \emph{Lipschitz condition}, which is the following: for column vectors $x$, $u$ in an open set $\mathcal{M}$ in a normal state space, there exists a constant $K$ such that

\begin{equation*}
\norm{F(x) - F(u)}_2 \leq K\norm{x - u}_2
\end{equation*}

For autonomous systems the Lipschitz condition guarantees both the existence of a solution and its uniqueness. We also have that ``if all partial derivatives $\partial F_i / \partial x_j$ are finite everywhere, then the function $F(x)$ satisfies the Lipschitz condition'' \cite[p. 677]{Haykin:2009:NNC:1213811}. Once again we note that the above results do not hold generally for nonautonomous systems: similar results hold only in certain neighborhoods of equilibrium states \cite[Definition A.2, p. 194]{rasmussen2006attractivity}.

\subsubsection{Stability around an equilibrium state}

The results in this subsection hold only for autonomous systems.

We say a constant vector $\overline{x} \in \mathcal{M}$ is a an \emph{equilibrium state} of the dynamic system if $F(\overline{x}) = 0$ (where of course 0 is a column vector). We examine the linearization of the state-space equation in a neighborhood of $\overline{x}$ (letting $\Delta x = x - \overline{x}$):

\begin{equation*}
x(t) = \overline{x} + \Delta x(t)
\end{equation*}

We consider the Taylor expansion of $F(x)$. We define the Jacobian as $J_xy = \frac{\partial y}{\partial x^T}$ where $x$ and $y$ are column vectors.

\begin{align*}
  \frac{d}{dt}x(t) & = F(x(t)) \\
  & = F(\overline{x}) + J_x F(\overline{x})(x - \overline{x}) + \cdots \\
  & = J_x F(\overline{x})(x - \overline{x}) + \cdots \\
  & \approx J_x F(\overline{x})(x - \overline{x}) = J_x F(\overline{x}) \Delta x
\end{align*}
%

Since

\begin{equation*}
  \frac{d}{dt}\Delta x = \frac{d}{dt} x - 0 = \frac{d}{dt} x
\end{equation*}
%
we have

\begin{equation*}
\frac{d}{dt}\Delta x(t) \approx J_x F(\overline{x}) \Delta x
\end{equation*}

If $J_x F(\overline{x})$ is invertible, we can solve for $\Delta x$ around $\overline{x}$, and the eigenvalues of $J_x F(\overline{x})$ determine the behavior of $\Delta x$ in this neighborhood.

Note that the reason the above linearization is not possible for nonautonomous systems is that if $F = F(x(t), t)$, taking the Jacobian with respect to only $x$ would ignore the direct effect of $t$ on the system (i.e. the effect of $t$ on $F$ that does not go through $x$).

\subsubsection{Types of stability of equilibrium states}

Again, the results in this subsection hold only for autonomous systems.

The above analysis is limited in the sense that it says nothing about whether or not the system ever enters the neighborhood of the equilibrium point in the first place. We would like to be able to say definitively under what circumstances an system will approach an equilibrium state. To do this, we have several definitions, i.e. types, of stability.

\begin{definition}
  The equilibrium state is said to be \emph{uniformly stable} if, for any positive constant $\epsilon$, there exists another positive constant $\delta = \delta(\epsilon)$ such that the condition
  \begin{equation*}
    \norm{x(0) - \overline{x}}_2 < \delta
  \end{equation*}
  implies 
  \begin{equation*}
    \norm{x(t) - \overline{x}}_2 < \epsilon
  \end{equation*}
  \cite[p. 681]{Haykin:2009:NNC:1213811}
\end{definition}

In other words, for a uniformly stable equilibrium state, we can guarantee that the state of the system at any time $t$ will be within an arbitrary distance of the equilibrium state provided that the starting state is within a certain distance from the equilibrium state.

\begin{definition}
  The equilibrium state is said to be \emph{convergent} if there exists a positive constant $\delta$ such that the condition
  \begin{equation*}
    \norm{x(0) - \overline{x}}_2 < \delta
  \end{equation*}
  implies 
  \begin{equation*}
    \lim_{t \to \infty} x(t) = \overline{x}
  \end{equation*}
  \cite[p. 681]{Haykin:2009:NNC:1213811}
\end{definition}

In other words, as long as the initial state is within a certain distance from the equilibrium state, the state of the system will eventually approach the equilibrium state \cite[p. 681]{Haykin:2009:NNC:1213811}.

\begin{definition}
  The equilibrium state is \emph{asymptotically stable} if it is both uniformly stable and convergent.
  \cite{wilson2010}
\end{definition}

\begin{definition}
  The equilibrium state is \emph{globally asymptotically stable} if it is both uniformly stable and all trajectories of the system converge to $\overline{x}$ as $t \to \infty$.
  \cite{wilson2010}
\end{definition}

Global asymptotic stability implies the system has only a single equilibrium state \cite[p. 681]{Haykin:2009:NNC:1213811}.

\subsubsection{Theorems regarding stability}

Now we present theorems, applicable only to autonomous systems, which give conditions under which stability of various types is achieved. These theorems are useful because they save us from having to solve for the solutions of the state-space equations. Rather, the theorems say that if we can find a certain function of the state variable that satisfies certain properties, then the equilibrium state is stable in a certain way. The theorems are part of the \emph{direct method of Lyapunov} \cite[p. 682]{Haykin:2009:NNC:1213811}, and the function $V(x)$ in the following theorems is called a Lyapunov function.

\begin{theorem}
The equilibrium state $\overline{x}$ is (Lyapunov) stable if, in a small neighborhood of $\overline{x}$ there exists a positive-definite function $V(x)$ such that its derivative with respect to time is negative semidefinite in that region.
\end{theorem}

\begin{theorem}
The equilibrium state $\overline{x}$ is asymptotically stable if, in a small neighborhood of $\overline{x}$, there exists a positive-definite function $V(x)$ such that its derivative with respect to time is negative definite in that region \cite[p. 682]{Haykin:2009:NNC:1213811}.
\end{theorem}

In the Haykin book, the first theorem omits the word ``Lyapunov'' and does not define ``stable'': he means ``Lyapunov stable.'' Note that in the definitions given, the initial time point is always taken to be $t_0 = 0$ so the distinction between Lyapunov stable and uniform stable is not made clear, and in any case, Lyapunov stability was not even defined. For the system to be Lyapunov stable, we have $\delta(\epsilon, t_0)$ where $\delta$ depends on $t_0$, whereas for the system to be uniform stable, the choice of $\delta$ is independent of $t_0$ \cite{byao}.

Letting $\mathcal{U}$ be the small neighborhood of $\overline{x}$, if $V(x)$ is a Lyapunov function, the equilibrium state $\overline{x}$ is Lyapunov stable if $\frac{d}{dt} V(x) \leq 0$ for $x \in \mathcal{U} - \overline{x}$ and is asymptotically stable if $\frac{d}{dt}V(x) < 0$ for $x \in \mathcal{U} - \overline{x}$. Since $\frac{d}{dt} V(x) \leq 0$ in a neighborhood, if we define a surface (called a \emph{Lyapunov surface}) $V(x) = c, c > 0$, once a trajectory crosses the surface, the trajectory stays within the set of points defined by $\{x \in \mathbb{R}^N : V(x) \leq c\}$ (note the mistake in Haykin here). If $\frac{d}{dt}V(x) < 0$, observe the trajectory continues to move closer and closer to $\overline{x}$, and by the second theorem, it approaches $\overline{x}$ as $t \to \infty$. (Note there is another mistake in Haykin: the book says ``we cannot be sure that the trajectory will actually converge onto $\overline{x}$ as $t \to \infty$'' but this is for the case of $\frac{d}{dt} V(x) \leq 0$, not strict inequality. For confirmation see, for example, \citealt[p. 18-19]{christofides2005control}.)

\subsection{Attractors}

Although the above exposition regarding Lyapunov surfaces is useful for intuitive understanding, there are several issues (leaving alone the fact that we are still only speaking of autonomous functions). One issue is we may not even have a Lyapunov function for our system. Since the existence of a Lyapunov function is a sufficient but not necessary condition for stability \cite[p. 683]{Haykin:2009:NNC:1213811}, it seems sensible to have a definition of a region of stability without explicit reference to Lyapunov functions. We define the \emph{basin of attraction} of an equilibrium state in the following manner:

\begin{definition}
The \emph{basin of attraction} of an equilibrium state $\overline{x}$ is the set of all $x$ such that $x(t) \to \overline{x}$ when $t \to \infty$ \cite{ocostin}.
\end{definition}

Note that this definition can be applied to nonautonomous systems as well. We also note that basins of attraction are usually defined with respect to \emph{attractors} of which there are four types, one of which is the equilibrium state (called an \emph{equilibrium point attractor} in this context \cite[p. 179]{DBLP:journals/ai/Beer95}). We restrict our consideration to equilibrium point attractors for the time being, which allows us to use the above more restricted definition.

We also observe an explicit connection between the Lyapunov surface and this more general concept of basin of attraction. For the small neighborhood $\mathcal{U}$ of $\overline{x}$, where $V(x)$ is negative definite, it can be shown that the sets $\mathcal{U}_c = \{x \in \mathcal{U} : V(x) \leq c\}$ are in the basin of attraction of $\overline{x}$ \cite{anovozhilov}.

\subsection{Information persistence}

A standard RNN has the mathematical form \cite[p. 381]{Goodfellow-et-al-2016}

\begin{align*}
  a(t) & = b + W h(t-1) + U x(t) \\
  h(t) & = \tanh(a(t)) \\
  o(t) & = c + V(h) \\
  \widehat{y}(t) & = \mbox{softmax}(o(t))
\end{align*}

When discussing stability for such a network, we are referring to the stability of the hidden unit $h$. For a sequence task, the hidden unit at each point in time should predict the desired output, and should incorporate information from as many previous time steps as will affect the desired output.

We consider

\begin{equation*}
h(t) = \tanh(a(t)) = \tanh(b + W h(t-1)) = M h(t-1)
\end{equation*}
%
where $M$ is the map taking $h(t - 1)$ to $h(t)$ for the system where $Ux(t)$ is not included (so the system is autonomous). If $h(0)$ begins within a basin of attraction then by our above analysis it will approach an attractor over time. In practice, if $h(0)$ and $W$ are initialized properly, we may be able to guarantee $h$ will begin in the correct basin of attraction (assuming the attractors even exist in the first place).

We summarize the argument of \citealt{DBLP:journals/tnn/BengioSF94} which is restated in \citealt{Hochreiter2001}. Assuming we are in the desired basin of attraction of a hyperbolic attractor (either an equilibrium point attractor or a periodic (limit cycle) attractor), if we consider only the autonomous system, the system will converge to the attractor. If the system is made nonautonomous by the inclusion of the additive term $Ux(t)$, as long as $Ux(t)$ is in a subset of the \emph{reduced attracting set} of the attractor, the system will still converge to the attractor \cite[p. 160]{DBLP:journals/tnn/BengioSF94}. However, if this condition is violated, the state of the hidden units may drift away from the attractor even if the state is still in the basin of attraction. \citeauthor*{DBLP:journals/tnn/BengioSF94} show that the reduced attracting set is defined as $\norm{J_x M} < 1$, i.e. the matrix norm of the Jacobian matrix of the mapping is less than 1 (for brevity, we ignore their discussion of uncertainty here). In short, when $Ux(t)$ is not in the reduced attracting set, the effect of such inputs $x(t)$ may be to push the state into a different basin of attraction than that of our desired hidden state. This may happen when, for example, the data is ``noisy,'' so that not all the information of $x(t)$ leads the model to converge to the desired attractor.

The authors draw our attention to a dilemma: in the region where $\norm{J_x M} < 1$, the gradient of the current state $h(t)$ with respect to $h(\tau), \tau \ll t$ decays quite rapidly \cite{Hochreiter2001}. This is an issue because gradient-based training methods, such as \emph{back-propagation through time} or BPTT, the update to the weight matrix $W$ is adjusted in proportion to these gradients. So, ironically, when the system is in the desired reduced attracting set, we cannot properly train the network, and when the system is not in that set, the weight matrix may change in undesired ways. Formally \cite{Hochreiter2001},

\begin{equation*}
  \frac{\partial E(t)}{\partial W} = \sum_{\tau \leq t} \frac{\partial E(t)}{\partial y(\tau)} \frac{\partial y(\tau)}{\partial W} = \sum_{\tau \leq t} \frac{\partial E(t)}{\partial y(t)} \frac{\partial y(t)}{\partial y(\tau)} \frac{\partial y(\tau)}{\partial W}
\end{equation*}
%
where for $s < t$,

\begin{equation*}
\frac{\partial y(t)}{\partial y(s)} = \frac{\partial y(t)}{\partial y(t-1)} \frac{\partial y(t-1)}{\partial y(t-2)} \cdots \frac{\partial y(s+1)}{\partial y(s)}
\end{equation*}

If the norm of each of the factors on the right hand side of this equation is less than 1, then $\frac{\partial y(t)}{\partial y(s)}$ converges to zero at an exponentially increasing rate as $t - s$ increases. Therefore looking at the equation for $\frac{\partial E(t)}{\partial W}$, for a single term of the summation with $\tau \ll t$,

\begin{equation*}
\biggl\lvert \frac{\partial E(t)}{\partial y(\tau)} \frac{\partial y(\tau)}{\partial W} \biggr\rvert \to 0
\end{equation*}

Why do we say ``we cannot properly train the network'' due to the above property of the model? Thinking independently of the given model structure for a moment, assume only the following: that we wish to miminize $E = \sum_{s} E(s)$, the sum of prediction errors of the sequence produced by the $y(0), \ldots, y(T)$, and that the value of any $y(t)$ is determined by the values of $y(0), \ldots, y(t-1)$. Assume that for the true model which we don't know, for a given $\tau \ll t$, $\frac{\partial y(t)}{\partial y(\tau)}$ is large, in other words the relationship between the state at time $\tau$ has a strong effect on the state at time $t$. When we specify a model of the sequence, we would like our training of our proposed model to reflect this strong ``dependency'' of the states at these two time points. Assume that making a particular change in $y(\tau)$ leads to a decrease in $E(t)$ that outweighs the sum of all increases in $E(s), s \neq t$. We would like the training of this model to be able to lead to this change.

Now assume the model we specify is the RNN model from above. In that model, making a change to $y(\tau)$ requires a particular update to $W$. However, the adjustment to the weight matrix corresponding to the dependency $\frac{\partial y(t)}{\partial y(\tau)}$ is now the product of gradients less than 1, which is essentially 0 for $\tau \ll t$. Therefore the model structure has forced the term that would allow the weights to update to reflect the desired change in $y(\tau)$ to have almost no effect on the weight update. The weights will never change to give the desired effect (we note in passing that such a change to $y(\tau)$ leading to a change in $y(t)$ corresponds to the trajectory changing to a different basin of attraction). This flaw in RNNs is known as the \emph{vanishing gradient problem}.

\section{iRNN}

Rectified linear units, or ReLUs, seem to have been introduced by \citealt{hahnloser2000digital}. It has been shown that the use of the ReLU as the activation function for an RNN allows the learning of long-term dependencies given that the weight matrix is initialized as a scalar multiple of the identity matrix \cite[p. 2]{DBLP:journals/corr/LeJH15}. The ReLU takes the form

\begin{equation*}
f(x) = \max(0, x)
\end{equation*}

\subsection{Training}

\citealt{DBLP:conf/nips/BaHMLI16} do not specify which cost function was used for training. Since a softmax function is used to produce the predicted values \cite[p. 5]{DBLP:conf/nips/BaHMLI16}, the predicted values can be interpreted as probabilities and we can thus use the cross-entropy cost function for the multiclass classification problem \cite{bishop2006pattern},

\begin{equation*}
-\sum_{n=1}^N \sum_{k=1}^K \widehat{y_{nk}}^{t_{nk}}
\end{equation*}

\section{The Hopfield network as an example of an autoassociative memory model}

We examine below the Hopfield network, a well-known model that is suited for the autoassociative memory task and which is an application of the additive model of the neuron.

\cite{Yegnanarayana:2004:ANN:1197006}.

\begin{table*}[t]
  \caption{Project timeline}
  \label{tab:freq}
  \begin{tabular}{llll}
    \toprule
    Week & Dates & Task & Deliverable\\
    \midrule
    Week 1 & (2/6   - 2/13) & Background reading & \\
    Week 2 & (2/13  - 2/20) & Background reading & Proposal\\
    Week 3 & (2/20  - 2/27) & Data set construction & \\
    Week 4 & (2/27  - 3/6)  & Background, foundational proofs & \\
    Week 5 & (3/6   - 3/13) & Partial implementation and preliminary run & \\
    Week 6 & (3/20  - 3/27) & Compose Intermediate Report & Intermediate Report\\
    Week 7 & (4/3   - 4/10) & Full implementation of networks & \\
    Week 8 & (4/10  - 4/17) & Complete empirical analysis & \\
    Week 9 & (4/17  - 4/24) & Compose Final Report & \\
    Week 10 & (4/24 - 5/1) & Finish Final Report and rehearse Presentation & Final Report \\
    & & & Presentation\\
  \bottomrule
\end{tabular}
\end{table*}

\section{Problem description}

Until recently, recurrent neural networks (RNNs), used for sequential processing, did not have a good way to share memory between sequences.
The Fast Weights method introduced in the paper to be analyzed in this project \cite{DBLP:conf/nips/BaHMLI16} addresses this limitation by providing the network with the capacity to store information about a given sequence during its duration to be used in the upcoming sequence. We will provide a full analysis and explanation of the methology, and replicate one of the empirical tests of the method, which compares its performance on an associative retrieval task to that of an iRNN and a long short-term memory network, or LSTM \cite{DBLP:journals/neco/HochreiterS97}.

\section{Survey of prior work}

Recurrent neural networks (RNNs) are well-suited for learning from sequential data since weights are shared among different stages of the sequence \cite[p. 373]{Goodfellow-et-al-2016}. In particular, RNNs have been shown to perform well in tasks of speech-to-text conversion, creation of language models for both characters and words \cite{DBLP:conf/icml/SutskeverMH11} and even frame by frame video analyses \cite{mnih}. In RNNs, a given hidden state essentially acts as short-term memory for the previous state, determining the next hidden state together with the next input. One major issue in training RNNs with long input sequences is that the error gradients end up becoming very large or small \cite[p. 16]{DBLP:journals/nn/Schmidhuber15} which implies that even if the network can be trained, the effect of an early hidden state on the current hidden state is practically non-existent. This problem was overcome by the introduction of the long short-term memory RNN (LSTM RNN), whose activation function has a constant derivative and thus does not explode or vanish \cite[p. 19]{DBLP:journals/nn/Schmidhuber15}. Unfortunately, the LSTM RNN's memory is still limited to an amount proportional to the number of hidden units in a sequence \cite[p. 1]{DBLP:conf/nips/BaHMLI16}. Ba et al. propose the Fast Weights method to allow sequence-to-sequence memory in a recurrent network. We also note that Hopfield nets \cite{MacKay:2002:ITI:971143} implemented a similar storage rule \cite[p. 2]{DBLP:conf/nips/BaHMLI16} which we will review in our paper.

\section{Preliminary plan}

Our term paper will first present the Fast Weights methodology and place it in the context of methods that led to its development. We will provide an extended description and derivation of the methodology for the purpose of verifying its properties. Our goal will be to also replicate Section 4.1 of the paper, which compares the Fast Weights' performance on an associative retrieval task with that of an Identity-RNN (iRNN) \cite{DBLP:journals/corr/TalathiV15}, and an LSTM RNN \cite{DBLP:conf/nips/BaHMLI16}.

This project is intended to verify the foundational math and reasoning which justify the use of Fast Weights in a network. This will require us to retrace the work leading up to the introduction of Fast Weights. The initial stage of our project will be to perform a thorough proof and derivation of the equations for RNNs, and clearly explain the issues that led to the creation of LSTM RNNs. For instance, we will explain the ``long-term memory issue'' in RNNs beginning as follows. The expression of the hidden unit $h_t$ at time $t$ is:

\begin{equation*}
  h_t = g(\vect{W} \cdot x_t + \vect{U} \cdot h_{t-1} + b_h)
\end{equation*}

After $t$ time steps, we get:

\begin{equation*}
  h_t = g(\vect{W} \cdot x_t + \vect{U} \cdot g(\cdots g(\vect{W} \cdot x_{t-T} + \vect{U} \cdot h_{t-T} + b_h) \cdots) + b_h)
\end{equation*}

Because of the $T$ nested multiplications of $h_{t-T}$ by $\vect{U}$, the effect of $h_{t-T}$ on $h_t$ is negligible (namely, the network does not have ``long-term memory''). We will provide a full exposition of how this problem manifests itself when the network is trained.

The next stage of the project will involve explaining LSTM RNNs, their improvements over RNNs, and their limitations. We will then explain the mathematics of Fast Weights in RNNs, as well as several methodologies used in their implementation in the paper being studied such as layer normalization \cite{DBLP:journals/corr/BaKH16}, grid search \cite{Goodfellow-et-al-2016}, and the Adam optimizer \cite{DBLP:journals/corr/KingmaB14}.

Following that, we will implement the Fast Associative Memory Network in MATLAB, and reproduce the analysis of Section 4.1 \cite{DBLP:conf/nips/BaHMLI16} of the paper to confirm the performance of Fast Weights as compared to the iRNN and the LSTM RNN.

\section{Memory and recurrent neural networks}

Paragraph about the neural perspective.

Paragraph about treating as signals, etc.

There are two major ways of incorporating the temporal element of sequential data in neural network learning. One is providing a memory structure that can present data from multiple time steps to a network that does not itself depend on the time variable (e.g. the time element is handled externally) \cite[p. 672-673]{Haykin:2009:NNC:1213811}. The other is incorporating the time element directly inside the network via the use of feedback. Feedback occurs in a system when the output of an element of the system eventually affects the input to that same element \cite[p. 18]{Haykin:2009:NNC:1213811}. There are two main types of feedback: local feedback and global feedback. Local feedback occurs when the output from an element directly feeds into that element's input, and global feedback occurs when the output eventually affects the input after passing through other elements first \cite[p. 673]{Haykin:2009:NNC:1213811}.

Recurrent neural networks are networks containing at least one feedback loop of either type \cite[p. 23]{Haykin:2009:NNC:1213811}. Feedback loops are inherently time-delay elements, where the output from an element is fed into a new element with a delay (in other words, transmission from one node to another is not instantaneous).

One major use of recurrent neural networks is to provide \emph{associative memory}. In the storage phase, an associative memory is presented with \emph{key patterns} and stores memorized patterns or \emph{values} which are implicitly associated with their key patterns. In the recall phase, when presented with a distorted or incomplete version of a key pattern, the memory produces the associated value pattern \cite[p. 38]{Haykin:2009:NNC:1213811}.

Let $s + l = N$, where $s$ is the total number of neurons used for storage of patterns and $l$ is the number of neurons that can be used for learning. One metric used when designing an associative memory is the ratio $q = \frac{s}{N} = \frac{s}{s + l}$. The total machine capacity ($N$) is limited to some value by resource constraints. Note that for a small $q$, the performance may be very good, though $s$ is small relative to $l$: in other words, the network must use a large number of neurons $l$ to recall very well only a few patterns. Therefore we seek to make $q$ as large as possible while still achieving good performance, i.e. the network should be able to recall correctly many patterns by using as few neurons for learning as possible \cite[p. 39]{Haykin:2009:NNC:1213811}.

There are two main types of associative memory, \emph{autoassociative} and \emph{heteroassociative memory}: in autoassociative memory, the memorized patterns are the same as the key patterns, whereas in heteroassociative memory, the memorized patterns are different \cite[p. 38]{Haykin:2009:NNC:1213811}. Different network structures may be more suited for one task than the other.

\subsection{A simple example of an associative memory model}

Consider an associative memory which learns the single key pattern $\vect{f}$ and value $\vect{g}$ where both are vectors (this example is from \cite[p. 163-165]{Anderson95}). We let the system be the matrix

\begin{equation*}
\vect{A} = \eta \vect{g}\vect{f}^T
\end{equation*}

The system performs perfectly:
%
\begin{equation*}
\vect{g}^\prime = \vect{A}\vect{f} = \eta\vect{g}\vect{f}^T\vect{f} \propto \vect{g}
\end{equation*}
%
since the $\vect{g}^\prime$ that is recalled is proportional to the value $\vect{g}$ associated with the input $\vect{f}$.

Now consider a set of key patterns $\vect{f}_i$ and associated values $\vect{g}_i$ where all $\vect{f}_i$ are orthogonal (we write $\vect{f}_i \rightarrow \vect{g}_i$ to denote the associations). Letting

\begin{equation*}
\vect{A}_i = \vect{g}_i \vect{f}_i^T, \qquad \vect{A} = \sum_{i} \vect{A}_i
\end{equation*}

We see that again $\vect{A}$ performs recall perfectly since for all $j$,

\begin{align*}
  \vect{A}\vect{f}_j & = \sum_{i}\vect{A}_i \vect{f}_j = \sum_{k \neq j} \vect{A}_k \vect{f}_j + \vect{A}_j\vect{f}_j \\
  & = \sum_{k \neq j} \vect{g}_k \vect{f}_k^T \vect{f}_j + \vect{g}_j = \vect{g}_j
\end{align*}

The above example suggests that outer products can be useful in constructing associative memory models. Using outer products to create memory storage is referred to ``the generalization of Hebb's postulate of learning'' \cite[p. 698]{Haykin:2009:NNC:1213811} since weight updates in Hebbian learning are calculated with outer products, as in the following example for a one-layer network \cite[p. 39-40]{fyfe2000}. For output $\vect{y} \in \mathbb{R}^m$ and input $\vect{x} \in \mathbb{R}^n$, if

\begin{equation*}
  y_i = \sum_{j} w_{ij} x_j, \qquad \Delta w_{ij} = \alpha x_j y_i
\end{equation*}

then

\begin{equation*}
\Delta w_{ij} = \alpha x_j \sum_k w_{ik} x_k = \alpha \sum_k w_{ik}x_j x_k
\end{equation*}

Letting $\alpha = \Delta t$,

\begin{equation*}
  \frac{\Delta w_{ij}}{\Delta t} = \sum_k w_{ik}x_j x_k
\end{equation*}

so as $\Delta t \rightarrow 0$,

\begin{equation*}
\frac{d}{dt}\vect{W}(t) = \vect{C}\vect{W}(t)
\end{equation*}

or, writing out the matrices fully,

\begin{equation*}
  \begin{split}
  \begin{bmatrix}
    \frac{dw_{11}}{dt} & \cdots & \frac{dw_{m1}}{dt} \\
    \vdots & \vdots & \vdots \\
    \frac{dw_{1n}}{dt} & \cdots & \frac{dw_{mn}}{dt} \\    
  \end{bmatrix} = \begin{bmatrix}
    x_1 x_1 & \cdots & x_n x_1 \\
    \vdots & \cdots & \vdots \\
    x_1 x_n & \cdots & x_n x_n 
  \end{bmatrix} \\
  & \hspace{-1.0cm}\begin{bmatrix}
    w_{11} & \cdots & w_{m1} \\
    \vdots & \cdots & \vdots \\
    w_{1n} & \cdots & w_{mn}
    \end{bmatrix}
  \end{split}
\end{equation*}

Note that the simple example of autoassociative memory would correspond to the weights matrix initially being the zero matrix and having only one update, after which the memory can recall perfectly. In general the linear system just described will not converge without some constraints being imposed \cite[p. 40]{fyfe2000}. Since it is known that given orthogonal key patterns only one weight update is needed, the algorithm can be run without considering issues of convergence or stability of the general case. However, stability of such systems is essential for learning algorithms based in such systems to exist; we will consider this point below in more detail.

Since not all sets of key values are orthogonal, the above system is inadequate for most cases. Before moving to other models we provide an overview of stability issues necessary for the design and evaluation of recurrent networks.

\section{Stability, controllability and observability in dynamic systems}

A \emph{dynamic system} is a system whose state changes with time \cite[p. 675]{Haykin:2009:NNC:1213811}. Above it was noted that feedback is a way of introducing time lags into a dynamic system. Depending on the particulars of the system, feedback can lead a system to stability or cause it to diverge. We consider a simple example \cite[p. 18-21]{Haykin:2009:NNC:1213811} of a dynamic system with feedback. From this example we will see that if the operator mapping input to output is a weight matrix, and outputs are mapped to inputs through a linear additive function, the values of the weights will determine whether the system is stable or diverges.

Consider the system defined by
%
\begin{equation*}
y_k(n) = wx_j^\prime(n), \qquad x_j^\prime(n) = x_j(n) + z^{-1}[y_k(n)]
\end{equation*}
%
where $z^{-1}$ is the unit time-delay operator, so $z^{-1}[y_k(n)] = y_k(n-1)$. Combining the two equations gives

\begin{equation*}
  y_k(n) = w \left(x_j(n) + z^{-1}[y_k(n)]\right)
\end{equation*}

\begin{equation*}
  y_k(n)(1 - wz^{-1}) = wx_j(n)
\end{equation*}

\begin{equation*}
  y_k(n) = w(1 - wz^{-1})^{-1}x_j(n)
\end{equation*}

Since
%
\begin{align*}
  \sum_{l=0}^{\infty}w^l z^{-l} & = \sum_{l=0}^{\infty}\left(\frac{w}{z}\right)^l = \frac{1}{(1 - wz^{-1})} \\
  & = (1 - wz^{-1})^{-1}
\end{align*}
%
we write
%
\begin{equation*}
  y_k(n) = w \sum_{l=0}^{\infty}w^l z^{-l}x_j(n) = \sum_{l=0}^{\infty}w^{l+1}x_j(n - l)
\end{equation*}
%
where we have applied the unit-time delay operator to the $x_j$ term.

We consider the case where $x_j(k)$ are sampled from, say, a Gaussian distribution whose positive mean is much smaller than the value of $x_j(0)$. We observe three cases:

\begin{enumerate}
\item If $|w| < 1$, the effect of the signal will decay towards $0$.
\item If $w = 1$, the system will diverge with the trend of $y_k(n)$ being linear. 
\item If $w > 1$, the system will diverge with the trend of $y_k(n)$ being exponential.  
\end{enumerate}

\subsection{The direct method of Lyanpunov and definitions of aspects of dynamic systems}

We note that system just described is a difference equation that is \emph{linear} (namely, the exponents of $x_j$ are to the first power only). Recurrent neural networks are generally \emph{nonlinear} (the exponents of $x_j$ are to powers greater than one) \cite[p. 6]{strogatz:2000}. To determine whether or not a general dynamic system (linear or nonlinear) is stable we utilize the direct method of Lyapunov \cite[p. 674]{Haykin:2009:NNC:1213811}. We present the method and important definitions here:

\begin{definition}
  State variables: $x_1(t), \ldots, x_N(t)$. Independent variable: $t$. Order of the system: $N$. State vector (in $N$-dimensions): $\vect{x}(t)$.
\end{definition}

A nonlinear dynamic system thus consists of the equations
%
\begin{equation*}
  \frac{d}{dt}x_j(t) = F_j \left(x_j(t)\right), \qquad j = 1, 2, \ldots, N
\end{equation*}
%
or
%
\begin{equation*}
  \frac{d}{dt}\vect{x}(t) = \vect{F} \left(\vect{x}(t)\right)
\end{equation*}
%
where $\vect{F}$ is non-linear and vector valued, with each element depending on the corresponding element of $\vect{x}(t)$. We assume that $\vect{F}$ only depends on $t$ through $\vect{x}$, in other words the system is \emph{autonomous} \cite[p. 675]{Haykin:2009:NNC:1213811}). The above equation is called the \emph{state-space} equation (ibid.). We wish to establish the existence and possibly uniqueness of solutions to the state-space equation.

$\vect{F}$ continuous is sufficient for the existence of a solution; for the uniqueness of the solution we require the \emph{Lipschitz condition}, which is the following: for $\vect{x}$, $\vect{u}$ in an open set $\mathcal{M}$ in a normal state space, there exists a constant $K$ such that

\begin{equation*}
\norm{\vect{F}(\vect{x}) - \vect{F}(\vect{u})}_2 \leq K\norm{\vect{x} - \vect{u}}_2
\end{equation*}

For autonomous systems the Lipschitz condition guarantees both the existence of a solution and its uniqueness. We also have that ``if all partial derivatives $\partial F_i / \partial x_j$ are finite everywhere, then the function $\vect{F}(\vect{x})$ satisfies the Lipschitz condition'' \cite[p. 677]{Haykin:2009:NNC:1213811}.

\subsubsection{Stability around an equilibrium state}

We say a constant vector $\overline{x} \in \mathcal{M}$ is a an \emph{equilibrium state} of the dynamic system if $F(\overline{x}) = \vect{0}$. We examine the linearization of the state-space equation in a neighborhood of $\overline{x}$:

\begin{equation*}
\vect{x}(t) = \overline{x} + \Delta x(t)
\end{equation*}

We consider the Taylor expansion of $\vect{F}(\vect{x})$. We define the Jacobian as $J_xy = \frac{\partial y}{\partial x^T}$ where $x$ and $y$ are column vectors.

\begin{align*}
  \frac{d}{dt}x(t) & = F(x(t)) \\
  & = F(\overline{x}) + J_x F(\overline{x})(x - \overline{x}) + \cdots \\
  & = J_x F(\overline{x})(x - \overline{x}) + \cdots \\
  & \approx J_x F(\overline{x})(x - \overline{x}) = J_x F(\overline{x}) \Delta x
\end{align*}
%
where $\Delta x = x - \overline{x}$.

Since

\begin{equation*}
  \frac{d}{dt}\Delta x = \frac{d}{dt} x - 0 = \frac{d}{dt} x
\end{equation*}

We have

\begin{equation*}
\frac{d}{dt}\Delta x(t) \approx J_x F(\overline{x}) \Delta x
\end{equation*}

If $J_x F(\overline{x})$ is invertible, we can solve for $\Delta x$ around $\overline{x}$, and the eigenvalues of $J_x F(\overline{x})$ determine the behavior of $\Delta x$ in this neighborhood.

\subsubsection{Stability of equilibrium states}

The above analysis is limited in the sense that it says nothing about whether or not the system ever enters the neighborhood of the equilibrium point in the first place. We would like to be able to say definitively under what circumstances an system will approach an equilibrium state. To do this, we have several definitions, i.e. types, of stability.

\begin{definition}
  The equilibrium state is said to be \emph{uniformly stable} if, for any positive constant $\epsilon$, there exists another positive constant $\delta = \delta(\epsilon)$ such that the condition

  \begin{equation*}
    \norm{x(0) - \overline{x}}_2 < \delta
  \end{equation*}
  implies
  \begin{equation*}
    \norm{x(t) - \overline{x}}_2 < \epsilon
  \end{equation*}
  \cite[p. 681]{Haykin:2009:NNC:1213811}
\end{definition}

In other words, for a uniformly stable equilibrium state, we can guarantee that the state of the system at any time $t$ will be within an arbitrary distance of the equilibrium state provided that the starting state is within a certain distance from the equilibrium state.

\section{The Hopfield network as an example of an autoassociative memory model}

We examine below the Hopfield network, a well-known model that is suited for the autoassociative memory task and which is an application of the additive model of the neuron.

\cite{Yegnanarayana:2004:ANN:1197006}.

\begin{table*}[t]
  \caption{Project timeline}
  \label{tab:freq}
  \begin{tabular}{llll}
    \toprule
    Week & Dates & Task & Deliverable\\
    \midrule
    Week 1 & (2/6   - 2/13) & Background reading & \\
    Week 2 & (2/13  - 2/20) & Background reading & Proposal\\
    Week 3 & (2/20  - 2/27) & Data set construction & \\
    Week 4 & (2/27  - 3/6)  & Background, foundational proofs & \\
    Week 5 & (3/6   - 3/13) & Partial implementation and preliminary run & \\
    Week 6 & (3/20  - 3/27) & Compose Intermediate Report & Intermediate Report\\
    Week 7 & (4/3   - 4/10) & Full implementation of networks & \\
    Week 8 & (4/10  - 4/17) & Complete empirical analysis & \\
    Week 9 & (4/17  - 4/24) & Compose Final Report & \\
    Week 10 & (4/24 - 5/1) & Finish Final Report and rehearse Presentation & Final Report \\
    & & & Presentation\\
  \bottomrule
\end{tabular}
\end{table*}
